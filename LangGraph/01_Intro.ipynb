{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a12ea6e",
   "metadata": {},
   "source": [
    "# Core Elements of LangGraph\n",
    "\n",
    "LangGraph is a library for building stateful, multi-actor applications with LLMs. It extends LangChain by allowing you to create cyclic graphs, which are essential for agentic workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analogy",
   "metadata": {},
   "source": [
    "### The Restaurant Analogy ðŸ½ï¸\n",
    "- **State**: The Order Ticket. Everyone (Chef, Waiter, Dishwasher) reads and updates it.\n",
    "- **Nodes**: The Staff. The waiter takes the order, the chef cooks the meal.\n",
    "- **Edges**: The Workflow. After the chef cooks, the ticket goes to the waiter.\n",
    "- **Conditional Edges**: The Decision. If the meal is ready, go to the customer; if not, wait.\n",
    "- **Checkpointer**: The Log Book. If the restaurant closes and reopens, the log book remembers where every order was left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1_state",
   "metadata": {},
   "source": [
    "## 1. State\n",
    "The `State` is the source of truth for your graph. It is a shared data structure defined by a schema (TypedDict or Pydantic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "\n",
    "# Option A: TypedDict (Standard)\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[str], operator.add]  # Appends to list instead of overwriting\n",
    "    count: int\n",
    "\n",
    "# Option B: Pydantic (Better for validation)\n",
    "class PydanticState(BaseModel):\n",
    "    query: str = Field(description=\"User input query\")\n",
    "    history: List[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2_nodes",
   "metadata": {},
   "source": [
    "## 2. Nodes\n",
    "Nodes are Python functions that take the current `State` and return a dictionary that *updates* the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "node_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_node(state: GraphState):\n",
    "    # Logic here (e.g., call an LLM)\n",
    "    new_message = \"Hello! How can I help?\"\n",
    "    return {\"messages\": [new_message], \"count\": state.get(\"count\", 0) + 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3_graph",
   "metadata": {},
   "source": [
    "## 3. Edges & Navigation\n",
    "Edges define how to move from one node to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edge_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"agent\", chatbot_node)\n",
    "\n",
    "# 1. Static Edge: Always go from START to 'agent'\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# 2. Conditional Edge: Decide where to go next based on state\n",
    "def should_continue(state: GraphState):\n",
    "    if state[\"count\"] > 5: return \"finish\"\n",
    "    return \"continue\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", \n",
    "    should_continue, \n",
    "    {\"finish\": END, \"continue\": \"agent\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4_command",
   "metadata": {},
   "source": [
    "## 4. Command (Modern Control Flow)\n",
    "In newer versions of LangGraph, you can use the `Command` object to handle both **state updates** and **navigation** directly from within a node, often eliminating the need for separate conditional edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "command_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "def smart_node(state: GraphState):\n",
    "    # Update state AND tell the graph where to go next\n",
    "    return Command(\n",
    "        update={\"messages\": [\"Thinking...\"]},\n",
    "        goto=\"agent\" if state[\"count\"] < 3 else END\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5_persistence",
   "metadata": {},
   "source": [
    "## 5. Persistence & Checkpointers\n",
    "LangGraph can save the state of the graph after every step. This allows for \"memory\" across different sessions and error recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistence_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Using a thread_id to identify a unique conversation\n",
    "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    "app.invoke({\"messages\": [\"Hi\"], \"count\": 0}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6_human_in_loop",
   "metadata": {},
   "source": [
    "## 6. Breakpoints (Human-in-the-loop)\n",
    "Breakpoints allow you to pause the graph execution before or after a specific node to wait for human approval or input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breakpoint_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the app with a breakpoint before the 'agent' node\n",
    "app = workflow.compile(checkpointer=memory, interrupt_before=[\"agent\"])\n",
    "\n",
    "# Execution will STOP here and wait for a command to resume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7_subgraphs",
   "metadata": {},
   "source": [
    "## 7. Subgraphs\n",
    "You can use a compiled graph as a node inside another graph. This is great for modularizing complex agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subgraph_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_workflow = StateGraph(GraphState)\n",
    "# ... define nodes/edges ...\n",
    "inner_app = inner_workflow.compile()\n",
    "\n",
    "parent_workflow = StateGraph(GraphState)\n",
    "parent_workflow.add_node(\"subgraph\", inner_app)\n",
    "parent_workflow.add_edge(START, \"subgraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8_tools",
   "metadata": {},
   "source": [
    "## 8. Tools & ToolNode\n",
    "Agents often need to use tools. LangGraph provides a prebuilt `ToolNode` to handle the execution of tool calls generated by models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str):\n",
    "    \"\"\"Get the current weather for a city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "tools = [get_weather]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Add it to your graph like any other node\n",
    "workflow.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9_prebuilt",
   "metadata": {},
   "source": [
    "## 9. Prebuilt Graphs (`create_react_agent`)\n",
    "For standard designs like a ReAct agent, LangGraph provides high-level factories to create them in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prebuilt_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "agent = create_react_agent(model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10_messages",
   "metadata": {},
   "source": [
    "## 10. Message Types\n",
    "Conversations are usually stored as a list of message objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "message_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"Paris.\")\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
